{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3013f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 导入与设备\n",
    "import os, time, random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# 实用函数\n",
    "def seed_everything(seed: int = 42, deterministic: bool = False) -> None:\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def count_params(model: nn.Module) -> Tuple[int, int]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_acc(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def run_one_epoch(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer | None = None,\n",
    "    train: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[float, float]:\n",
    "    model.train(train)\n",
    "    epoch_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    return epoch_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= 新增版本 2：BN-only（有BN，无残差） =============\n",
    "class BNCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    基于 SimpleCNN 的骨干，在每个卷积后、激活前加入 BatchNorm\n",
    "    不包含任何残差连接\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=10, img_size=28):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, img_size, img_size)\n",
    "            flat_dim = self.features(dummy).view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57ce470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载（与你提供的实现一致）\n",
    "# =========================\n",
    "def make_loaders(\n",
    "    name: str,\n",
    "    root: str = \"./data\",\n",
    "    batch: int = 128,\n",
    "    val_ratio: float = 0.1,\n",
    "    workers: int = 2,\n",
    "    pin: bool = False,\n",
    ") -> Dict[str, object]:\n",
    "    name_l = name.lower()\n",
    "    if name_l == \"mnist\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        train_full = datasets.MNIST(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.MNIST(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 1, 28\n",
    "    elif name_l == \"cifar10\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        train_full = datasets.CIFAR10(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.CIFAR10(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 3, 32\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset name.\")\n",
    "\n",
    "    val_size = int(len(train_full) * val_ratio)\n",
    "    train_size = len(train_full) - val_size\n",
    "    train_ds, val_ds = random_split(\n",
    "        train_full, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    def mk(ds, bs, shuffle):\n",
    "        return DataLoader(\n",
    "            ds, batch_size=bs, shuffle=shuffle,\n",
    "            num_workers=workers, pin_memory=pin\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": mk(train_ds, batch, True),\n",
    "        \"val\":   mk(val_ds, batch * 2, False),\n",
    "        \"test\":  mk(test_ds, batch * 2, False),\n",
    "        \"in_channels\": in_channels,\n",
    "        \"img_size\": img_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffd9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置与例行函数（与你提供的版本一致）\n",
    "# =========================\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 15,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 5e-4,\n",
    "    ckpt_path: str = \"best.pt\",\n",
    "    use_plateau: bool = True,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5,\n",
    "                                  patience=2, min_lr=1e-5) if use_plateau else None\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_patience = 6\n",
    "    patience = early_patience\n",
    "    history = {\"train_loss\": [], \"train_acc\": [],\n",
    "               \"val_loss\": [], \"val_acc\": [], \"lrs\": [], \"time\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ep_start = time.time()\n",
    "\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            train_loader, model, criterion, optimizer,\n",
    "            train=True, device=device\n",
    "        )\n",
    "        val_loss, val_acc = run_one_epoch(\n",
    "            val_loader, model, criterion, optimizer=None,\n",
    "            train=False, device=device\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lrs\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        history[\"time\"].append(time.time() - ep_start)\n",
    "\n",
    "        print(f\"[{os.path.basename(ckpt_path)}] Epoch {ep:02d}/{epochs} | \"\n",
    "              f\"time {history['time'][-1]:.1f}s | \"\n",
    "              f\"Train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "              f\"Val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "              f\"LR {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save({\"model\": model.state_dict()}, ckpt_path)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # 加载最佳权重\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9757e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 实验主入口（四个模型，可逐个运行）\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)\n",
    "\n",
    "    # 统一实验配置：MNIST 10 epochs，CIFAR10 25 epochs（控制变量）\n",
    "    cfg = {\n",
    "        \"mnist\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "        \"cifar10\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "    }\n",
    "\n",
    "    # 你可以按需注释任何一段，避免全部训练耗时\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d016aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MNIST ----------\n",
    "mnist = make_loaders(\"mnist\", root=\"./data\")\n",
    "in_ch_m, img_m = mnist[\"in_channels\"], mnist[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434b26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST-BNCNN params: (50378, 50378)\n",
      "[mnist_bn_best.pt] Epoch 01/10 | time 7.1s | Train 0.1485/0.9555 | Val 0.0707/0.9787 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 02/10 | time 4.7s | Train 0.0500/0.9850 | Val 0.0735/0.9782 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 03/10 | time 4.8s | Train 0.0387/0.9882 | Val 0.0518/0.9845 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 04/10 | time 4.7s | Train 0.0311/0.9903 | Val 0.0506/0.9852 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 05/10 | time 4.7s | Train 0.0264/0.9915 | Val 0.0491/0.9858 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 06/10 | time 4.7s | Train 0.0221/0.9931 | Val 0.0538/0.9842 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 07/10 | time 4.6s | Train 0.0196/0.9940 | Val 0.0526/0.9833 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 08/10 | time 4.6s | Train 0.0187/0.9942 | Val 0.0481/0.9868 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 09/10 | time 4.5s | Train 0.0180/0.9943 | Val 0.0482/0.9867 | LR 1.0e-03\n",
      "[mnist_bn_best.pt] Epoch 10/10 | time 4.4s | Train 0.0158/0.9953 | Val 0.0473/0.9863 | LR 1.0e-03\n",
      "BNCNN on MNIST - Test Accuracy: 0.9888, Total Training Time: 48.96 seconds\n"
     ]
    }
   ],
   "source": [
    "# BNCNN\n",
    "model = BNCNN(in_ch_m, 10, img_m).to(device)\n",
    "print(\"MNIST-BNCNN params:\", count_params(model))\n",
    "history = train_model(model, mnist[\"train\"], mnist[\"val\"], device,\n",
    "                    epochs=cfg[\"mnist\"][\"epochs\"], lr=cfg[\"mnist\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"mnist\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./mnist_bn_best.pt\")\n",
    "acc = evaluate_acc(model, mnist[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"BNCNN on MNIST - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# 如需保存到 results（可选）\n",
    "results[\"mnist_bn\"] = (acc, count_params(model)[0], total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1755a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CIFAR10 ----------\n",
    "cifar = make_loaders(\"cifar10\", root=\"./data\")\n",
    "in_ch_c, img_c = cifar[\"in_channels\"], cifar[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e34d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10-BNCNN params: (60554, 60554)\n",
      "[cifar_bn_best.pt] Epoch 01/10 | time 4.7s | Train 1.3374/0.5301 | Val 1.0964/0.6232 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 02/10 | time 4.7s | Train 0.9813/0.6563 | Val 0.9391/0.6864 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 03/10 | time 4.5s | Train 0.8774/0.6949 | Val 0.9272/0.6780 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 04/10 | time 4.7s | Train 0.8144/0.7186 | Val 0.9296/0.6710 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 05/10 | time 4.7s | Train 0.7593/0.7375 | Val 0.8390/0.7098 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 06/10 | time 4.4s | Train 0.7076/0.7567 | Val 0.8506/0.7088 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 07/10 | time 4.2s | Train 0.6708/0.7690 | Val 0.8823/0.6916 | LR 1.0e-03\n",
      "[cifar_bn_best.pt] Epoch 08/10 | time 4.3s | Train 0.6350/0.7801 | Val 0.9249/0.6904 | LR 5.0e-04\n",
      "[cifar_bn_best.pt] Epoch 09/10 | time 4.4s | Train 0.5416/0.8172 | Val 0.7877/0.7316 | LR 5.0e-04\n",
      "[cifar_bn_best.pt] Epoch 10/10 | time 4.4s | Train 0.5199/0.8260 | Val 0.8215/0.7202 | LR 5.0e-04\n",
      "BNCNN on CIFAR10 - Test Accuracy: 0.7329, Total Training Time: 45.04 seconds\n"
     ]
    }
   ],
   "source": [
    " # BNCNN\n",
    "model = BNCNN(in_ch_c, 10, img_c).to(device)\n",
    "print(\"CIFAR10-BNCNN params:\", count_params(model))\n",
    "history = train_model(model, cifar[\"train\"], cifar[\"val\"], device,\n",
    "                    epochs=cfg[\"cifar10\"][\"epochs\"], lr=cfg[\"cifar10\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"cifar10\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./cifar_bn_best.pt\")\n",
    "acc = evaluate_acc(model, cifar[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"BNCNN on CIFAR10 - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# 如需保存到 results（可选）\n",
    "results[\"cifar_bn\"] = (acc, count_params(model)[0], total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
