{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc63236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 导入与设备\n",
    "import os, time, random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# 实用函数\n",
    "def seed_everything(seed: int = 42, deterministic: bool = False) -> None:\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def count_params(model: nn.Module) -> Tuple[int, int]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_acc(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def run_one_epoch(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer | None = None,\n",
    "    train: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[float, float]:\n",
    "    model.train(train)\n",
    "    epoch_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    return epoch_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacb8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResBlock（原增强版）：Conv-BN-ReLU -> Conv-BN -> +identity -> ReLU\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 增强版：ResCNN（含残差与BN）——与你提供的实现一致\n",
    "class ResCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, img_size=28):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # block 1\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # -> 32 x H/2 x W/2\n",
    "\n",
    "            # residual block at same channels/size\n",
    "            ResBlock(32),\n",
    "\n",
    "            # block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # -> 64 x H/4 x W/4\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, img_size, img_size)\n",
    "            flat_dim = self.features(dummy).view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb72c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载（与你提供的实现一致）\n",
    "# =========================\n",
    "def make_loaders(\n",
    "    name: str,\n",
    "    root: str = \"./data\",\n",
    "    batch: int = 128,\n",
    "    val_ratio: float = 0.1,\n",
    "    workers: int = 2,\n",
    "    pin: bool = False,\n",
    ") -> Dict[str, object]:\n",
    "    name_l = name.lower()\n",
    "    if name_l == \"mnist\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        train_full = datasets.MNIST(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.MNIST(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 1, 28\n",
    "    elif name_l == \"cifar10\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        train_full = datasets.CIFAR10(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.CIFAR10(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 3, 32\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset name.\")\n",
    "\n",
    "    val_size = int(len(train_full) * val_ratio)\n",
    "    train_size = len(train_full) - val_size\n",
    "    train_ds, val_ds = random_split(\n",
    "        train_full, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    def mk(ds, bs, shuffle):\n",
    "        return DataLoader(\n",
    "            ds, batch_size=bs, shuffle=shuffle,\n",
    "            num_workers=workers, pin_memory=pin\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": mk(train_ds, batch, True),\n",
    "        \"val\":   mk(val_ds, batch * 2, False),\n",
    "        \"test\":  mk(test_ds, batch * 2, False),\n",
    "        \"in_channels\": in_channels,\n",
    "        \"img_size\": img_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a28922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置与例行函数（与你提供的版本一致）\n",
    "# =========================\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 15,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 5e-4,\n",
    "    ckpt_path: str = \"best.pt\",\n",
    "    use_plateau: bool = True,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5,\n",
    "                                  patience=2, min_lr=1e-5) if use_plateau else None\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_patience = 6\n",
    "    patience = early_patience\n",
    "    history = {\"train_loss\": [], \"train_acc\": [],\n",
    "               \"val_loss\": [], \"val_acc\": [], \"lrs\": [], \"time\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ep_start = time.time()\n",
    "\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            train_loader, model, criterion, optimizer,\n",
    "            train=True, device=device\n",
    "        )\n",
    "        val_loss, val_acc = run_one_epoch(\n",
    "            val_loader, model, criterion, optimizer=None,\n",
    "            train=False, device=device\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lrs\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        history[\"time\"].append(time.time() - ep_start)\n",
    "\n",
    "        print(f\"[{os.path.basename(ckpt_path)}] Epoch {ep:02d}/{epochs} | \"\n",
    "              f\"time {history['time'][-1]:.1f}s | \"\n",
    "              f\"Train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "              f\"Val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "              f\"LR {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save({\"model\": model.state_dict()}, ckpt_path)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # 加载最佳权重\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf99c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 实验主入口（四个模型，可逐个运行）\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)\n",
    "\n",
    "    # 统一实验配置：MNIST 10 epochs，CIFAR10 25 epochs（控制变量）\n",
    "    cfg = {\n",
    "        \"mnist\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "        \"cifar10\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "    }\n",
    "\n",
    "    # 你可以按需注释任何一段，避免全部训练耗时\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df07b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MNIST ----------\n",
    "mnist = make_loaders(\"mnist\", root=\"./data\")\n",
    "in_ch_m, img_m = mnist[\"in_channels\"], mnist[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f33ea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST-ResCNN params: (69002, 69002)\n",
      "[mnist_res_best.pt] Epoch 01/10 | time 6.4s | Train 0.1276/0.9610 | Val 0.0602/0.9833 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 02/10 | time 4.4s | Train 0.0405/0.9874 | Val 0.0658/0.9818 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 03/10 | time 4.8s | Train 0.0332/0.9894 | Val 0.0404/0.9877 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 04/10 | time 4.3s | Train 0.0254/0.9918 | Val 0.0402/0.9880 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 05/10 | time 4.3s | Train 0.0225/0.9926 | Val 0.0403/0.9882 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 06/10 | time 4.4s | Train 0.0192/0.9939 | Val 0.0363/0.9892 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 07/10 | time 4.3s | Train 0.0159/0.9952 | Val 0.0483/0.9853 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 08/10 | time 4.3s | Train 0.0157/0.9949 | Val 0.0363/0.9883 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 09/10 | time 4.4s | Train 0.0165/0.9945 | Val 0.0448/0.9868 | LR 1.0e-03\n",
      "[mnist_res_best.pt] Epoch 10/10 | time 4.4s | Train 0.0131/0.9958 | Val 0.0299/0.9898 | LR 1.0e-03\n",
      "MNIST-ResCNN on CIFAR10 - Test Accuracy: 0.9928, Total Training Time: 45.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# ResCNN\n",
    "model = ResCNN(in_ch_m, 10, img_m).to(device)\n",
    "print(\"MNIST-ResCNN params:\", count_params(model))\n",
    "history = train_model(model, mnist[\"train\"], mnist[\"val\"], device,\n",
    "                    epochs=cfg[\"mnist\"][\"epochs\"], lr=cfg[\"mnist\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"mnist\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./mnist_res_best.pt\")\n",
    "acc = evaluate_acc(model, mnist[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"MNIST-ResCNN on CIFAR10 - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "results[\"mnist_res\"] = (acc, count_params(model)[0], total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CIFAR10 ----------\n",
    "cifar = make_loaders(\"cifar10\", root=\"./data\")\n",
    "in_ch_c, img_c = cifar[\"in_channels\"], cifar[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81585608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10-ResCNN params: (79178, 79178)\n",
      "[cifar_res_best.pt] Epoch 01/10 | time 4.5s | Train 1.3143/0.5365 | Val 1.0642/0.6312 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 02/10 | time 4.4s | Train 0.9236/0.6794 | Val 0.9934/0.6478 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 03/10 | time 4.3s | Train 0.7857/0.7282 | Val 0.8345/0.7086 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 04/10 | time 4.4s | Train 0.7077/0.7548 | Val 0.8097/0.7150 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 05/10 | time 4.4s | Train 0.6444/0.7765 | Val 0.7753/0.7314 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 06/10 | time 4.4s | Train 0.5873/0.7967 | Val 0.7200/0.7518 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 07/10 | time 4.4s | Train 0.5501/0.8095 | Val 0.7740/0.7348 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 08/10 | time 4.8s | Train 0.5057/0.8260 | Val 0.7955/0.7348 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 09/10 | time 4.4s | Train 0.4710/0.8363 | Val 0.7042/0.7598 | LR 1.0e-03\n",
      "[cifar_res_best.pt] Epoch 10/10 | time 4.5s | Train 0.4402/0.8482 | Val 0.7112/0.7524 | LR 1.0e-03\n",
      "MNIST-ResCNN on CIFAR10 - Test Accuracy: 0.7618, Total Training Time: 44.42 seconds\n"
     ]
    }
   ],
   "source": [
    " # ResCNN\n",
    "model = ResCNN(in_ch_c, 10, img_c).to(device)\n",
    "print(\"CIFAR10-ResCNN params:\", count_params(model))\n",
    "history = train_model(model, cifar[\"train\"], cifar[\"val\"], device,\n",
    "                    epochs=cfg[\"cifar10\"][\"epochs\"], lr=cfg[\"cifar10\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"cifar10\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./cifar_res_best.pt\")\n",
    "acc = evaluate_acc(model, cifar[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"MNIST-ResCNN on CIFAR10 - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "results[\"cifar_res\"] = (acc, count_params(model)[0], total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
