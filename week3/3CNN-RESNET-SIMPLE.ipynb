{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26f5c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 导入与设备\n",
    "import os, time, random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# 实用函数\n",
    "def seed_everything(seed: int = 42, deterministic: bool = False) -> None:\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def count_params(model: nn.Module) -> Tuple[int, int]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_acc(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def run_one_epoch(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer | None = None,\n",
    "    train: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[float, float]:\n",
    "    model.train(train)\n",
    "    epoch_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    return epoch_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92ac9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualOnlyBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    去除 BN：Conv -> ReLU -> Conv -> +identity -> ReLU\n",
    "    通道与尺寸均保持与输入一致\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)     # 无BN，卷积后直接ReLU\n",
    "        out = self.conv2(out)    # 第二个卷积后无BN\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualOnlyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    基于 ResCNN 的骨干，但彻底移除所有 BatchNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=10, img_size=28):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # block 1（移除BN）\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # -> 32 x H/2 x W/2\n",
    "\n",
    "            # residual block（无BN版本）\n",
    "            ResidualOnlyBlock(32),\n",
    "\n",
    "            # block 2（移除BN）\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # -> 64 x H/4 x W/4\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, img_size, img_size)\n",
    "            flat_dim = self.features(dummy).view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17038f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载（与你提供的实现一致）\n",
    "# =========================\n",
    "def make_loaders(\n",
    "    name: str,\n",
    "    root: str = \"./data\",\n",
    "    batch: int = 128,\n",
    "    val_ratio: float = 0.1,\n",
    "    workers: int = 2,\n",
    "    pin: bool = False,\n",
    ") -> Dict[str, object]:\n",
    "    name_l = name.lower()\n",
    "    if name_l == \"mnist\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        train_full = datasets.MNIST(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.MNIST(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 1, 28\n",
    "    elif name_l == \"cifar10\":\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        train_full = datasets.CIFAR10(root, train=True, download=True, transform=tfm)\n",
    "        test_ds = datasets.CIFAR10(root, train=False, download=True, transform=tfm)\n",
    "        in_channels, img_size = 3, 32\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset name.\")\n",
    "\n",
    "    val_size = int(len(train_full) * val_ratio)\n",
    "    train_size = len(train_full) - val_size\n",
    "    train_ds, val_ds = random_split(\n",
    "        train_full, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    def mk(ds, bs, shuffle):\n",
    "        return DataLoader(\n",
    "            ds, batch_size=bs, shuffle=shuffle,\n",
    "            num_workers=workers, pin_memory=pin\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": mk(train_ds, batch, True),\n",
    "        \"val\":   mk(val_ds, batch * 2, False),\n",
    "        \"test\":  mk(test_ds, batch * 2, False),\n",
    "        \"in_channels\": in_channels,\n",
    "        \"img_size\": img_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d4f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置与例行函数（与你提供的版本一致）\n",
    "# =========================\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 15,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 5e-4,\n",
    "    ckpt_path: str = \"best.pt\",\n",
    "    use_plateau: bool = True,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5,\n",
    "                                  patience=2, min_lr=1e-5) if use_plateau else None\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_patience = 6\n",
    "    patience = early_patience\n",
    "    history = {\"train_loss\": [], \"train_acc\": [],\n",
    "               \"val_loss\": [], \"val_acc\": [], \"lrs\": [], \"time\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ep_start = time.time()\n",
    "\n",
    "        tr_loss, tr_acc = run_one_epoch(\n",
    "            train_loader, model, criterion, optimizer,\n",
    "            train=True, device=device\n",
    "        )\n",
    "        val_loss, val_acc = run_one_epoch(\n",
    "            val_loader, model, criterion, optimizer=None,\n",
    "            train=False, device=device\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lrs\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        history[\"time\"].append(time.time() - ep_start)\n",
    "\n",
    "        print(f\"[{os.path.basename(ckpt_path)}] Epoch {ep:02d}/{epochs} | \"\n",
    "              f\"time {history['time'][-1]:.1f}s | \"\n",
    "              f\"Train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "              f\"Val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "              f\"LR {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save({\"model\": model.state_dict()}, ckpt_path)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # 加载最佳权重\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec90e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 实验主入口（四个模型，可逐个运行）\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)\n",
    "\n",
    "    # 统一实验配置：MNIST 10 epochs，CIFAR10 25 epochs（控制变量）\n",
    "    cfg = {\n",
    "        \"mnist\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "        \"cifar10\": {\"epochs\": 10, \"lr\": 1e-3, \"weight_decay\": 5e-4},\n",
    "    }\n",
    "\n",
    "    # 你可以按需注释任何一段，避免全部训练耗时\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942836f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MNIST ----------\n",
    "mnist = make_loaders(\"mnist\", root=\"./data\")\n",
    "in_ch_m, img_m = mnist[\"in_channels\"], mnist[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e836dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST-ResidualOnlyCNN params: (68682, 68682)\n",
      "[mnist_resonly_best.pt] Epoch 01/10 | time 6.7s | Train 0.1752/0.9472 | Val 0.0733/0.9795 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 02/10 | time 4.4s | Train 0.0488/0.9852 | Val 0.0520/0.9847 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 03/10 | time 4.6s | Train 0.0376/0.9884 | Val 0.0465/0.9858 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 04/10 | time 4.6s | Train 0.0300/0.9906 | Val 0.0435/0.9878 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 05/10 | time 4.4s | Train 0.0255/0.9918 | Val 0.0446/0.9862 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 06/10 | time 4.4s | Train 0.0236/0.9929 | Val 0.0392/0.9878 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 07/10 | time 4.5s | Train 0.0197/0.9938 | Val 0.0425/0.9873 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 08/10 | time 4.5s | Train 0.0181/0.9942 | Val 0.0383/0.9870 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 09/10 | time 4.7s | Train 0.0181/0.9939 | Val 0.0378/0.9883 | LR 1.0e-03\n",
      "[mnist_resonly_best.pt] Epoch 10/10 | time 5.2s | Train 0.0165/0.9947 | Val 0.0338/0.9905 | LR 1.0e-03\n",
      "ResidualOnlyCNN on MNIST - Test Accuracy: 0.9906, Total Training Time: 48.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# ResidualOnlyCNN\n",
    "model = ResidualOnlyCNN(in_ch_m, 10, img_m).to(device)\n",
    "print(\"MNIST-ResidualOnlyCNN params:\", count_params(model))\n",
    "history = train_model(model, mnist[\"train\"], mnist[\"val\"], device,\n",
    "                    epochs=cfg[\"mnist\"][\"epochs\"], lr=cfg[\"mnist\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"mnist\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./mnist_resonly_best.pt\")\n",
    "acc = evaluate_acc(model, mnist[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"ResidualOnlyCNN on MNIST - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "results[\"mnist_resonly\"] = (acc, count_params(model)[0], total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1efec261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CIFAR10 ----------\n",
    "cifar = make_loaders(\"cifar10\", root=\"./data\")\n",
    "in_ch_c, img_c = cifar[\"in_channels\"], cifar[\"img_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a40517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10-ResidualOnlyCNN params: (78858, 78858)\n",
      "[cifar_resonly_best.pt] Epoch 01/10 | time 4.6s | Train 1.4076/0.4999 | Val 1.1321/0.6052 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 02/10 | time 5.0s | Train 1.0295/0.6407 | Val 0.9776/0.6552 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 03/10 | time 4.5s | Train 0.8717/0.6962 | Val 0.9060/0.6854 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 04/10 | time 4.4s | Train 0.7788/0.7312 | Val 0.8523/0.7016 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 05/10 | time 4.5s | Train 0.6996/0.7592 | Val 0.8237/0.7164 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 06/10 | time 4.5s | Train 0.6373/0.7802 | Val 0.7710/0.7282 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 07/10 | time 4.9s | Train 0.5891/0.7963 | Val 0.8076/0.7200 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 08/10 | time 4.4s | Train 0.5429/0.8135 | Val 0.7911/0.7294 | LR 1.0e-03\n",
      "[cifar_resonly_best.pt] Epoch 09/10 | time 4.4s | Train 0.5029/0.8252 | Val 0.7883/0.7334 | LR 5.0e-04\n",
      "[cifar_resonly_best.pt] Epoch 10/10 | time 5.1s | Train 0.4067/0.8635 | Val 0.7872/0.7396 | LR 5.0e-04\n",
      "ResidualOnlyCNN on CIFAR10 - Test Accuracy: 0.7352, Total Training Time: 46.24 seconds\n"
     ]
    }
   ],
   "source": [
    "# ResidualOnlyCNN\n",
    "model = ResidualOnlyCNN(in_ch_c, 10, img_c).to(device)\n",
    "print(\"CIFAR10-ResidualOnlyCNN params:\", count_params(model))\n",
    "history = train_model(model, cifar[\"train\"], cifar[\"val\"], device,\n",
    "                    epochs=cfg[\"cifar10\"][\"epochs\"], lr=cfg[\"cifar10\"][\"lr\"],\n",
    "                    weight_decay=cfg[\"cifar10\"][\"weight_decay\"],\n",
    "                    ckpt_path=\"./cifar_resonly_best.pt\")\n",
    "acc = evaluate_acc(model, cifar[\"test\"], device)\n",
    "total_time = sum(history[\"time\"])\n",
    "print(f\"ResidualOnlyCNN on CIFAR10 - Test Accuracy: {acc:.4f}, Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "results[\"cifar_resonly\"] = (acc, count_params(model)[0], total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
