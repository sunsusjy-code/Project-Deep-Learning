import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
import functools

# æ£€æŸ¥ç¯å¢ƒ
try:
    from torch.func import vmap, grad, jacrev, functional_call
    print("âœ… ä½¿ç”¨ PyTorch 2.0+ åŸç”Ÿ torch.func")
except ImportError:
    from functorch import vmap, grad, jacrev, make_functional
    print("âš ï¸ ä½¿ç”¨ functorch (æ—§ç‰ˆ)")

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.set_default_dtype(torch.float64) # LM å¯¹ç²¾åº¦è¦æ±‚æé«˜ï¼Œå…¨ç¨‹ä½¿ç”¨ double
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

# ==========================================
# 1. é‡‘èç¯å¢ƒä¸æ•°æ®ç”Ÿæˆ (God Mode)
# ==========================================
REAL_SIGMA = 0.20  # çœŸå®æ³¢åŠ¨ç‡ 20%
RISK_FREE_RATE = 0.05

def black_scholes_formula(S, K, T, r, sigma):
    # æ ‡å‡†æ¬§å¼çœ‹æ¶¨æœŸæƒå®šä»·å…¬å¼
    import scipy.stats as si
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    call_val = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))
    return torch.tensor(call_val, dtype=torch.float64, device=device)

# ç”Ÿæˆâ€œå¸‚åœºæ•°æ®â€ (Market Data)
# æˆ‘ä»¬ç”Ÿæˆ 100 ä¸ªè§‚æµ‹ç‚¹
N_obs = 100
K_strike = 100.0

S_obs = (torch.rand(N_obs, 1, device=device) * 50.0 + 75.0) # è‚¡ä»· 75~125
t_obs = (torch.rand(N_obs, 1, device=device) * 0.9 + 0.1)   # æ—¶é—´ 0.1~1.0
X_obs = torch.cat([S_obs, t_obs], dim=1)

# è®¡ç®—çœŸå®ä»·æ ¼
V_market = black_scholes_formula(S_obs.cpu().numpy(), K_strike, t_obs.cpu().numpy(), RISK_FREE_RATE, REAL_SIGMA).squeeze()

# ç”Ÿæˆ PDE é…ç‚¹ (Collocation Points)
# æˆ‘ä»¬ç”Ÿæˆ 200 ä¸ª PDE ç‚¹
N_pde = 200
S_pde = (torch.rand(N_pde, 1, device=device) * 50.0 + 75.0).requires_grad_(True)
t_pde = (torch.rand(N_pde, 1, device=device) * 0.9 + 0.1).requires_grad_(True)
X_pde = torch.cat([S_pde, t_pde], dim=1)

print(f"âœ… æ•°æ®å‡†å¤‡å®Œæ¯•: {N_obs} å¸‚åœºç‚¹, {N_pde} PDE ç‚¹")

# ==========================================
# 2. å®šä¹‰å¾®å‹ç½‘ç»œ (Tiny Model)
# ==========================================
# ä¸ºäº†è®© LM è·‘å¾—åŠ¨ï¼Œæˆ‘ä»¬ç”¨æç®€ç½‘ç»œ
class TinyPricingModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 10), nn.Tanh(),
            nn.Linear(10, 10), nn.Tanh(),
            nn.Linear(10, 1)
        )
    def forward(self, x):
        return self.net(x)

model = TinyPricingModel().to(device)

# --- å‚æ•°æ‰å¹³åŒ– (Flattening) ---
# æˆ‘ä»¬éœ€è¦æŠŠ (ç½‘ç»œæƒé‡ + sigma) æ‹¼æˆä¸€ä¸ªé•¿å‘é‡ theta
# 1. æå–ç½‘ç»œæƒé‡
params_dict = dict(model.named_parameters())
param_names = list(params_dict.keys())
param_shapes = [p.shape for p in params_dict.values()]
param_numels = [p.numel() for p in params_dict.values()]

# 2. åˆå§‹åŒ– Sigma (ççŒœä¸€ä¸ªå€¼)
init_sigma = 0.5 # çŒœ 50% (çœŸå®æ˜¯ 20%)
sigma_param = torch.tensor([np.log(init_sigma)], device=device, requires_grad=True) # ä½¿ç”¨ log ä¿è¯æ­£æ•°

# 3. æ‹¼åˆæ‰€æœ‰å‚æ•°åˆ°ä¸€ä¸ªå‘é‡
# è¿™æ˜¯ä¸€ä¸ª helper å‡½æ•°ï¼ŒæŠŠ list è½¬æˆ vector
def params_to_vector(params_dict, sigma_tensor):
    vecs = [p.flatten() for p in params_dict.values()]
    vecs.append(sigma_tensor.flatten())
    return torch.cat(vecs)

# 4. åå‘ helperï¼šæŠŠ vector æ‹†å› (params_dict, sigma)
def vector_to_params(vec):
    # æ‹†è§£ç½‘ç»œæƒé‡
    new_params = {}
    idx = 0
    for i, name in enumerate(param_names):
        count = param_numels[i]
        new_params[name] = vec[idx : idx+count].view(param_shapes[i])
        idx += count
    # æ‹†è§£ sigma
    new_sigma = vec[idx:]
    return new_params, new_sigma

# åˆå§‹å‚æ•°å‘é‡ theta_0
theta_init = params_to_vector(params_dict, sigma_param).detach().requires_grad_(True)
print(f"ğŸ“¦ å‚æ•°æ€»æ•°é‡ (Weights + Sigma): {theta_init.numel()}")

# ==========================================
# 3. å®šä¹‰æ®‹å·®å‘é‡å‡½æ•° (The Big Residual)
# ==========================================
# è¿™ä¸ªå‡½æ•°è¾“å…¥ä¸€ä¸ªå·¨å¤§çš„ theta å‘é‡ï¼Œè¾“å‡ºä¸€ä¸ªå·¨å¤§çš„æ®‹å·®å‘é‡ [r_data, r_pde]

def get_all_residuals(theta, x_obs_batch, v_obs_batch, x_pde_batch):
    # A. æ‹†åŒ…å‚æ•°
    curr_params, curr_sigma_log = vector_to_params(theta)
    sigma = torch.exp(curr_sigma_log) # è¿˜åŸ sigma
    
    # --- B. è®¡ç®— Data Residuals (æ‹Ÿåˆå¸‚åœºä»·æ ¼) ---
    # å®šä¹‰ä¸´æ—¶çš„ forward å‡½æ•°
    def forward_func(p, x):
        return functional_call(model, p, (x,)).squeeze()
    
    v_pred = forward_func(curr_params, x_obs_batch)
    res_data = v_pred - v_obs_batch
    
    # --- C. è®¡ç®— PDE Residuals (Black-Scholes) ---
    # å®šä¹‰å•ç‚¹ PDE è®¡ç®—å‡½æ•°
    def pde_step(p, x_single):
        # ä¸€é˜¶å¯¼: dV/dS, dV/dt (x[0]=S, x[1]=t)
        grads = grad(forward_func, argnums=1)(p, x_single)
        V_S, V_t = grads[0], grads[1]
        
        # äºŒé˜¶å¯¼: d2V/dS2
        hess = jacrev(grad(forward_func, argnums=1), argnums=1)(p, x_single)
        V_SS = hess[0, 0]
        
        S_val = x_single[0]
        V_val = forward_func(p, x_single)
        
        # Black-Scholes æ®‹å·®
        # V_t + 0.5 * sigma^2 * S^2 * V_SS + r * S * V_S - r * V = 0
        f = V_t + 0.5 * (sigma**2) * (S_val**2) * V_SS + RISK_FREE_RATE * S_val * V_S - RISK_FREE_RATE * V_val
        return f

    # æ‰¹é‡è®¡ç®— PDE æ®‹å·®
    res_pde = vmap(pde_step, in_dims=(None, 0))(curr_params, x_pde_batch)
    
# ============ âš ï¸ ä¿®æ”¹è¿™é‡Œ ============
    # å¼ºåˆ¶æŠŠå®ƒä»¬éƒ½å˜æˆ 1ç»´å‘é‡ (flatten)
    # è¿™æ ·èƒ½é˜²æ­¢ [100] å’Œ [200, 1] è¿™ç§ç»´åº¦æ‰“æ¶çš„æƒ…å†µ
    res_data = res_data.reshape(-1)
    res_pde = res_pde.reshape(-1)
    # ====================================

    # D. æ‹¼æ¥æ‰€æœ‰æ®‹å·®
    # æˆ‘ä»¬ç»™ Data Residual åŠ ç‚¹æƒé‡ (æ¯”å¦‚ x10)ï¼Œå› ä¸ºå®ƒæ›´é‡è¦
    return torch.cat([res_data * 10.0, res_pde])

# ==========================================
# 4. LM ä¼˜åŒ–ä¸»å¾ªç¯ (Full LM)
# ==========================================
print("\nğŸš€ å¼€å§‹å…¨é‡ LM ä¼˜åŒ– (åŒæ—¶ä¼˜åŒ– Weights å’Œ Sigma)...")

theta = theta_init.clone()
mu = 1.0 # åˆå§‹é˜»å°¼
loss_history = []
sigma_history = []

t0 = time.time()

for i in range(50): # LM æ”¶æ•›æå¿«ï¼Œ50æ¬¡è¶³å¤Ÿ
    # 1. è®¡ç®—é›…å¯æ¯”çŸ©é˜µ J å’Œæ®‹å·® r
    # J çš„å½¢çŠ¶: [N_samples, N_params] -> [300, 141] å·¦å³
    
    # è®¡ç®— r
    r = get_all_residuals(theta, X_obs, V_market, X_pde)
    mse = torch.mean(r**2).item()
    
    # è®°å½•å½“å‰ sigma
    _, curr_sig_log = vector_to_params(theta)
    curr_sig = torch.exp(curr_sig_log).item()
    sigma_history.append(curr_sig)
    loss_history.append(mse)
    
    print(f"Iter {i:2d} | Loss: {mse:.6f} | ğŸ•µï¸ Sigma: {curr_sig:.4f} (Target: {REAL_SIGMA}) | mu: {mu:.1e}")
    
    if mse < 1e-6:
        print("ğŸ‰ æ”¶æ•›è¾¾æˆï¼")
        break
        
    # è®¡ç®— J (è¿™æ˜¯æœ€è€—æ—¶çš„ä¸€æ­¥)
    # jacrev å¯¹ç¬¬ä¸€ä¸ªå‚æ•°(theta)æ±‚å¯¼
    J = jacrev(get_all_residuals, argnums=0)(theta, X_obs, V_market, X_pde)
    
    # --- LM æ›´æ–°æ­¥ ---
    H = J.T @ J
    g = J.T @ r
    P = theta.shape[0]
    
    # ç®€å•çš„ LM æ›´æ–°é€»è¾‘ (çœç•¥äº†å›æº¯æ­¥ï¼Œç®€åŒ–æ¼”ç¤º)
    # theta_new = theta - (H + mu*I)^-1 @ g
    try:
        delta = torch.linalg.solve(H + mu * torch.eye(P, device=device), -g)
        theta = theta + delta
        
        # æ¿€è¿›ç­–ç•¥ï¼šæ¯æ¬¡æˆåŠŸéƒ½å‡å° mu (é€¼è¿‘é«˜æ–¯ç‰›é¡¿)
        mu = max(1e-7, mu / 2.0)
        
    except RuntimeError:
        # å¦‚æœçŸ©é˜µå¥‡å¼‚ï¼Œå¢å¤§ mu (å›é€€åˆ°æ¢¯åº¦ä¸‹é™)
        print("âš ï¸ çŸ©é˜µå¥‡å¼‚ï¼Œå¢åŠ é˜»å°¼...")
        mu = mu * 10.0

t1 = time.time()
print(f"\nâœ… è®­ç»ƒç»“æŸï¼è€—æ—¶: {t1-t0:.2f}ç§’")
print(f"æœ€ç»ˆ Sigma é¢„æµ‹: {sigma_history[-1]:.5f}")
print(f"çœŸå® Sigma: {REAL_SIGMA}")
print(f"è¯¯å·®: {abs(sigma_history[-1] - REAL_SIGMA)/REAL_SIGMA*100:.2f}%")

# ç”»å›¾
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(loss_history)
plt.title('Loss Convergence (Full LM)')
plt.yscale('log')

plt.subplot(1,2,2)
plt.plot(sigma_history)
plt.axhline(REAL_SIGMA, color='r', linestyle='--')
plt.title('Sigma Calibration')
plt.savefig('full_lm_result.png')