import math
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.func import vmap, grad, jacrev, functional_call

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.set_default_dtype(torch.float64)
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device} | ç­–ç•¥: æš´åŠ›è¿‡æ‹Ÿåˆ(Overfit) + è¾…åŠ©è½®(Clamp)")

# ==========================================
# 1. æ•°æ®å¢å¼º (åŠ å¤§æ•°æ®é‡)
# ==========================================
REAL_SIGMA = 0.20
RISK_FREE_RATE = 0.05
K_strike = 100.0
# ğŸŸ¢ ä¿®æ”¹ç‚¹ 1: å¢åŠ æ•°æ®ç‚¹ï¼Œè®©ç½‘ç»œæ²¡æ³•â€œå·æ‡’â€
N_obs = 300  
N_pde = 1000

def black_scholes_formula(S, K, T, r, sigma):
    import scipy.stats as si
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    call_val = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))
    return torch.tensor(call_val, dtype=torch.float64, device=device)

# æ•°æ®ç”Ÿæˆ
S_obs = (torch.rand(N_obs, 1, device=device) * 50.0 + 75.0)
t_obs = (torch.rand(N_obs, 1, device=device) * 0.9 + 0.1)
X_obs = torch.cat([S_obs, t_obs], dim=1)
V_market = black_scholes_formula(S_obs.cpu().numpy(), K_strike, t_obs.cpu().numpy(), RISK_FREE_RATE, REAL_SIGMA).squeeze()

S_pde = (torch.rand(N_pde, 1, device=device) * 50.0 + 75.0).requires_grad_(True)
t_pde = (torch.rand(N_pde, 1, device=device) * 0.9 + 0.1).requires_grad_(True)
X_pde = torch.cat([S_pde, t_pde], dim=1)

# ==========================================
# 2. æ¨¡å‹å®šä¹‰
# ==========================================
class OptionPricingModel_ALT(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¨å¾®åŠ æ·±ä¸€ç‚¹ç½‘ç»œï¼Œä¿è¯æ‹Ÿåˆèƒ½åŠ›
        self.net = nn.Sequential(
            nn.Linear(2, 32, dtype=torch.float64), nn.Tanh(),
            nn.Linear(32, 32, dtype=torch.float64), nn.Tanh(),
            nn.Linear(32, 1, dtype=torch.float64)
        )
        self.act_out = nn.Softplus()
        # åˆå§‹ç›²çŒœ 0.5
        self.sigma_log = nn.Parameter(torch.tensor([np.log(0.5)], dtype=torch.float64))

    def forward(self, x):
        return self.act_out(self.net(x)).squeeze()
    
    def get_sigma(self):
        return torch.exp(self.sigma_log)

model = OptionPricingModel_ALT().to(device)

# å‚æ•°ç®¡ç†
all_params_list = list(model.parameters())
param_numels = [p.numel() for p in all_params_list]
param_shapes = [p.shape for p in all_params_list]
param_names = [name for name, p in model.named_parameters()]

def vector_to_params_dict(vec):
    new_params_list = []
    idx = 0
    for i, count in enumerate(param_numels):
        new_params_list.append(vec[idx : idx+count].view(param_shapes[i]))
        idx += count
    return dict(zip(param_names, new_params_list))

# ==========================================
# 3. æ ¸å¿ƒå‡½æ•° (Jacobianç­‰ä¿æŒæ ‡å‡†ç‰ˆ)
# ==========================================
def compute_jacobian_chunked(theta, x_o, v_m, x_p, chunk_size=50):
    def func_data(t):
        p = vector_to_params_dict(t)
        return (functional_call(model, p, (x_o,)).squeeze() - v_m).view(-1) * 10.0
    J_data = jacrev(func_data)(theta)
    
    J_pde_list = []
    N = x_p.shape[0]
    for i in range(0, N, chunk_size):
        x_chunk = x_p[i:i+chunk_size]
        def func_pde(t):
            p = vector_to_params_dict(t)
            s_val = torch.exp(p['sigma_log'])
            r_rate = RISK_FREE_RATE
            def step(x_s):
                def inner(x): return functional_call(model, p, (x.unsqueeze(0),)).squeeze()
                g = grad(inner)(x_s)
                h = jacrev(grad(inner))(x_s)
                return g[1] + r_rate*x_s[0]*g[0] + 0.5*(s_val**2)*(x_s[0]**2)*h[0,0] - r_rate*inner(x_s)
            return vmap(step)(x_chunk).view(-1)
        torch.cuda.empty_cache()
        J_chunk = jacrev(func_pde)(theta) 
        J_pde_list.append(J_chunk)
    return torch.cat([J_data, torch.cat(J_pde_list, dim=0)], dim=0)

def get_all_residuals(theta_vector, x_obs, v_market, x_pde):
    curr_params = vector_to_params_dict(theta_vector)
    sigma_val = torch.exp(curr_params['sigma_log'])
    r_rate = RISK_FREE_RATE
    v_pred = functional_call(model, curr_params, (x_obs,)).squeeze()
    res_data = (v_pred - v_market) * 10.0
    def pde_step(x_s):
        def inner(x): return functional_call(model, curr_params, (x.unsqueeze(0),)).squeeze()
        g = grad(inner)(x_s)
        h = jacrev(grad(inner))(x_s)
        V_S, V_t = g[0], g[1]
        V_SS = h[0,0]
        return g[1] + r_rate*x_s[0]*g[0] + 0.5*(sigma_val**2)*(x_s[0]**2)*h[0,0] - r_rate*inner(x_s)
    res_pde = vmap(pde_step)(x_pde)
    return torch.cat([res_data.view(-1), res_pde.view(-1)])


# ==========================================
# 4. ä¼˜åŒ–æµç¨‹ (ä¿®æ­£ç‰ˆ)
# ==========================================

# --- Phase 1: æš´åŠ›è¿‡æ‹Ÿåˆ Data ---
# ç›®æ ‡ï¼šå¿…é¡»æŠŠ Data Loss å‹åˆ° 1e-3 ä»¥ä¸‹ï¼Œå¦åˆ™å½¢çŠ¶å°±æ˜¯é”™çš„
print("\nğŸ”¥ Phase 1: æš´åŠ›æ‹Ÿåˆ Data (Lock Sigma)...")
model.sigma_log.requires_grad = False
optimizer_net = torch.optim.Adam(model.net.parameters(), lr=0.01)

for i in range(3000): # ğŸŸ¢ å¢åŠ æ­¥æ•°
    optimizer_net.zero_grad()
    
    v_pred = model(X_obs)
    loss_data = torch.mean((v_pred - V_market)**2)
    
    loss_data.backward()
    optimizer_net.step()
    
    if i % 500 == 0:
        print(f"Iter {i}: Data Loss {loss_data.item():.6f} | Sigma {model.get_sigma().item():.4f}")

print(f"âœ… Phase 1 ç»“æŸ. æœ€ç»ˆ Data Loss: {loss_data.item():.6f}")
if loss_data.item() > 0.01:
    print("âš ï¸ è­¦å‘Šï¼šæ•°æ®æ‹Ÿåˆä¾ç„¶å¾ˆå·®ï¼Œåç»­åæ¼”å¯èƒ½ä¼šå¤±è´¥ï¼")


# --- Phase 2: è”åˆè®­ç»ƒ (å¸¦è¾…åŠ©è½®) ---
# æˆ‘ä»¬ä¸å†å•ç‹¬è®­ç»ƒ Sigmaï¼Œè€Œæ˜¯è”åˆè®­ç»ƒï¼Œä½†æ˜¯ç»™ Sigma åŠ ä¸Šâ€œè¾…åŠ©è½®â€ (Clamp)
print("\nğŸ”¥ Phase 2: è”åˆè®­ç»ƒ (Joint with Clamp)...")
model.sigma_log.requires_grad = True
optimizer_all = torch.optim.Adam(model.parameters(), lr=0.005)

for i in range(2000):
    optimizer_all.zero_grad()
    
    # 1. è®¡ç®—è”åˆ Loss
    # Data éƒ¨åˆ†
    v_pred = model(X_obs)
    loss_data = torch.mean((v_pred - V_market)**2) * 100.0 # ä¿æŒé«˜æƒé‡
    
    # PDE éƒ¨åˆ†
    sigma = model.get_sigma()
    r_rate = RISK_FREE_RATE
    v_pde = model(X_pde)
    grads = torch.autograd.grad(v_pde, X_pde, torch.ones_like(v_pde), create_graph=True)[0]
    V_S, V_t = grads[:, 0:1], grads[:, 1:2]
    grads_2 = torch.autograd.grad(V_S, X_pde, torch.ones_like(V_S), create_graph=True)[0]
    V_SS = grads_2[:, 0:1]
    S_val = X_pde[:, 0:1]
    f = V_t + r_rate * S_val * V_S + 0.5 * (sigma**2) * (S_val**2) * V_SS - r_rate * v_pde
    loss_pde = torch.mean(f**2)
    
    loss = loss_data + loss_pde
    loss.backward()
    optimizer_all.step()
    
    # ğŸŸ¢ å…³é”®ç‚¹ï¼šæ¯æ¬¡æ›´æ–°å®Œï¼Œå¼ºåˆ¶æŠŠ Sigma æ‹‰å›åˆç†åŒºé—´
    # é˜²æ­¢ Adam å·æ‡’è·‘åˆ° 0 å»ã€‚æˆ‘ä»¬å‡è®¾æ³¢åŠ¨ç‡è‡³å°‘æ˜¯ 0.1
    with torch.no_grad():
        model.sigma_log.data.clamp_(min=math.log(0.1))

    if i % 200 == 0:
        print(f"Iter {i}: Loss {loss.item():.4e} | Sigma {model.get_sigma().item():.4f}")

print("âœ… Phase 2 ç»“æŸï¼Œè¿›å…¥ LM...")

# --- Phase 3: LM ç»ˆæå¾®è°ƒ ---
print("\nğŸš€ Phase 3: LM ç»ˆæå¾®è°ƒ...")
mu = 1e-1
max_lm_steps = 20

for i in range(max_lm_steps):
    theta_lm = nn.utils.parameters_to_vector(model.parameters())
    try:
        J = compute_jacobian_chunked(theta_lm, X_obs, V_market, X_pde, chunk_size=100)
        with torch.no_grad():
            r = get_all_residuals(theta_lm, X_obs, V_market, X_pde)
            loss_val = torch.mean(r**2)
        H = J.T @ J
        g = J.T @ r
        A = H + mu * torch.eye(theta_lm.shape[0], device=device).double()
        delta = torch.linalg.solve(A, -g)
        
        if torch.norm(delta) > 1.0: delta = delta / torch.norm(delta)
        
        with torch.no_grad():
            ptr = 0
            for p in model.parameters():
                num = p.numel()
                p.data += delta[ptr:ptr+num].view(p.shape)
                ptr += num
        
        # LM é˜¶æ®µä¹Ÿå¯ä»¥åŠ ä¸ªç®€å•çš„ Clampï¼Œé˜²æ­¢é£å‡ºå®‡å®™
        with torch.no_grad():
            model.sigma_log.data.clamp_(min=math.log(0.05), max=math.log(2.0))
            
        sig = model.get_sigma().item()
        mu = max(1e-6, mu / 3.0)
        print(f"LM {i}: Loss {loss_val:.4e} | Sigma {sig:.5f}")
            
    except Exception as e:
        print(f"Error: {e}")
        break

print(f"\næœ€ç»ˆç»“æœ: {model.get_sigma().item():.5f} (çœŸå®: {REAL_SIGMA})")