import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
import functools

# æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ–°çš„ torch.func (PyTorch 2.0+)
# æ•™æˆçš„ PPT ç”¨çš„æ˜¯æ—§ç‰ˆ functorch [cite: 417]ï¼Œæˆ‘ä»¬å°½é‡å…¼å®¹
try:
    from torch.func import vmap, grad, jacrev, functional_call
    print("âœ… ä½¿ç”¨ PyTorch 2.0+ åŸç”Ÿ torch.func")
except ImportError:
    from functorch import vmap, grad, jacrev, make_functional
    print("âš ï¸ ä½¿ç”¨ functorch (æ—§ç‰ˆ)ï¼Œå¦‚æœæŠ¥é”™è¯·å‡çº§ PyTorch")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

# --- 1. å®šä¹‰ç‰©ç†é—®é¢˜ (Problem Setup) ---
# å¯¹åº” PPT [cite: 648-650]
# æˆ‘ä»¬è¦è§£æ–¹ç¨‹: -Laplacian(u) = f
# è®¾å®šçœŸå®è§£ u = x^2 + y^2ï¼Œé‚£ä¹ˆ f å¿…é¡»ç­‰äº -4

def exact_u(x, y):
    k = 1.0
    return (x**2 + y**2) / k

def get_f_value(x, y):
    return -4.0

print("âœ… æ¨¡å—ä¸€ï¼šç‰©ç†é—®é¢˜å®šä¹‰å®Œæˆ")
# --- 2. å®šä¹‰ç¥ç»ç½‘ç»œä¸å‡½æ•°åŒ– (Model & Functionalization) ---
# å¯¹åº” PPT [cite: 194-198]

class Plain(nn.Module):
    def __init__(self, in_dim, h_dim, out_dim):
        super().__init__()
        # æ•™æˆ PPT ç”¨çš„æ˜¯ double ç²¾åº¦ï¼Œæˆ‘ä»¬ä¹Ÿä¿æŒä¸€è‡´ä»¥æé«˜ LM ç¨³å®šæ€§
        self.ln1 = nn.Linear(in_dim, h_dim).double()
        self.act1 = nn.Tanh()
        self.ln2 = nn.Linear(h_dim, h_dim).double()
        self.act2 = nn.Tanh()
        self.ln3 = nn.Linear(h_dim, out_dim).double()

    def forward(self, x):
        out = self.ln1(x)
        out = self.act1(out)
        out = self.ln2(out)
        out = self.act2(out)
        out = self.ln3(out)
        return out

# åˆå§‹åŒ–æ¨¡å‹
# 2ä¸ªè¾“å…¥(x,y) -> 20ä¸ªéšè—ç¥ç»å…ƒ -> 1ä¸ªè¾“å‡º(u)
num_neuron = 20
model = Plain(2, num_neuron, 1).to(device)

# --- æ ¸å¿ƒé»‘ç§‘æŠ€ï¼šå‡½æ•°åŒ– (Functionalization) ---
# æˆ‘ä»¬éœ€è¦æŠŠå‚æ•°(params)å’Œæ¨¡å‹ç»“æ„(func_model)åˆ†ç¦»å¼€
# è¿™æ ·æ‰èƒ½å¯¹ params æ±‚å¯¼è®¡ç®— Jacobian

# æå–å‚æ•° (params)
params = dict(model.named_parameters())

# å®šä¹‰ä¸€ä¸ª"çº¯å‡½æ•°"ç‰ˆæœ¬çš„æ¨¡å‹ forward
# è¾“å…¥: params, x
# è¾“å‡º: u
# --- ä¿®æ­£åçš„ fnet_single ---
def fnet_single(params, x):
    # ä¸éœ€è¦æ‰‹åŠ¨ unsqueezeï¼Œå› ä¸º nn.Linear å¯ä»¥å¤„ç†å•ä¸ªå‘é‡è¾“å…¥
    # è¾“å…¥ x: [2] -> è¾“å‡º: [1]
    out = functional_call(model, params, (x,))
    return out.squeeze() # ç¡®ä¿è¿”å›çš„æ˜¯æ ‡é‡(scalar)ï¼Œè¿™å¯¹ grad å¾ˆé‡è¦

print("âœ… æ¨¡å—äºŒï¼šç¥ç»ç½‘ç»œå®šä¹‰ & å‡½æ•°åŒ–å®Œæˆ")
print(f"   - å‚æ•°æ•°é‡: {sum(p.numel() for p in params.values())}")

# --- 3. æ•°æ®ç”Ÿæˆ & æ®‹å·®è®¡ç®— (Data & Residuals) ---
# [cite_start]å¯¹åº” PPT [cite: 673-675, 697-698]

# 3.1 ç”Ÿæˆé‡‡æ ·ç‚¹ (Collocation Points)
# ç®€å•çš„å‡åŒ€ç½‘æ ¼ (ç®€å•èµ·è§ï¼Œæš‚ä¸ç”¨ Chebyshev)
# --- ä¿®æ­£åçš„æ•°æ®ç”Ÿæˆ ---
cnt = 10
x_range = torch.linspace(-1, 1, cnt)
y_range = torch.linspace(-1, 1, cnt)
X, Y = torch.meshgrid(x_range, y_range, indexing='ij')

# âš ï¸ å…³é”®ä¿®æ”¹ï¼šåŠ ä¸Š .double()
x_pde = torch.stack([X.flatten(), Y.flatten()], dim=-1).to(device).double()

# 3.2 å®šä¹‰å•ä¸ªç‚¹çš„æ®‹å·®å‡½æ•° (Single Point Residual)
# æ³¨æ„ï¼šè¿™é‡Œè¾“å…¥æ˜¯å•ä¸ªç‚¹ x (shape: [2])ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ ‡é‡æ®‹å·®
def compute_pde_residual_single(params, x):
    # è®¡ç®— u å¯¹ x çš„ä¸€é˜¶å¯¼ (Gradient) -> [du/dx, du/dy]
    # argnums=1 è¡¨ç¤ºå¯¹ç¬¬äºŒä¸ªå‚æ•° x æ±‚å¯¼
    grads = grad(fnet_single, argnums=1)(params, x)
    
    # è®¡ç®— u å¯¹ x çš„äºŒé˜¶å¯¼ (Hessian) -> [[u_xx, u_xy], [u_yx, u_yy]]
    # ä¹Ÿå°±æ˜¯å¯¹ grads å†æ±‚ä¸€æ¬¡å¯¼
    hess = jacrev(grad(fnet_single, argnums=1), argnums=1)(params, x)
    
    # æå– Laplacian: u_xx + u_yy
    # hess[0,0] æ˜¯ u_xx, hess[1,1] æ˜¯ u_yy
    u_xx = hess[0, 0]
    u_yy = hess[1, 1]
    laplacian = u_xx + u_yy
    
    # PDE: -Delta u = f  =>  -Delta u - f = 0
    # æˆ‘ä»¬è®¾å®šçš„ f æ˜¯ -4 (get_f_value)
    # æ‰€ä»¥æ®‹å·® = -laplacian - (-4)
    target_f = -4.0
    res = -laplacian - target_f
    return res

print("âœ… æ¨¡å—ä¸‰ï¼šæ®‹å·®å‡½æ•°å®šä¹‰å®Œæˆ")

# --- éªŒè¯ä¸€ä¸‹ vmap æ˜¯å¦å·¥ä½œ ---
# vmap å…è®¸æˆ‘ä»¬å°†"å•ç‚¹å‡½æ•°"è‡ªåŠ¨å˜æˆ"æ‰¹é‡å‡½æ•°"
# in_dims=(None, 0) è¡¨ç¤º: params ä¸å˜(None), x æŒ‰ç¬¬0ç»´æ‰¹å¤„ç†
batch_residual_fn = vmap(compute_pde_residual_single, in_dims=(None, 0))

# è¯•ç€ç®—ä¸€æ¬¡æ®‹å·®å‘é‡
r_vector = batch_residual_fn(params, x_pde)
print(f"   - é‡‡æ ·ç‚¹æ•°é‡: {x_pde.shape[0]}")
print(f"   - æ®‹å·®å‘é‡å½¢çŠ¶: {r_vector.shape} (åº”è¯¥ç­‰äºé‡‡æ ·ç‚¹æ•°é‡)")
print(f"   - åˆå§‹ MSE Loss: {torch.mean(r_vector**2).item():.6f}")

# --- 4. LM ä¼˜åŒ–å™¨å®ç° (Levenberg-Marquardt Optimization) ---
# [cite_start]å¯¹åº” PPT [cite: 563, 574-577, 701-706]

# 4.1 è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æ®‹å·®å‘é‡ r å’Œ é›…å¯æ¯”çŸ©é˜µ J
def get_r_and_J(params, x):
    # è®¡ç®—æ®‹å·®å‘é‡ r (shape: [N])
    r = batch_residual_fn(params, x)
    
    # è®¡ç®—é›…å¯æ¯”å­—å…¸ J_dict
    # jacrev ä¼šè¿”å›ä¸€ä¸ªå’Œ params ç»“æ„ä¸€æ ·çš„å­—å…¸
    # å­—å…¸é‡Œæ¯ä¸ªå€¼çš„ shape æ˜¯ (N, param_shape)
    J_dict = jacrev(batch_residual_fn)(params, x)
    
    # --- æ ¸å¿ƒï¼šæŠŠå­—å…¸æ‹æ‰æˆçŸ©é˜µ J (shape: [N, P]) ---
    J_list = []
    for name, val in J_dict.items():
        # val.shape: (N, d1, d2...) -> view -> (N, d1*d2...)
        # ä¾‹å¦‚ (100, 20, 20) -> (100, 400)
        N = val.shape[0]
        J_list.append(val.view(N, -1))
    
    # åœ¨åˆ—ç»´åº¦æ‹¼æ¥ï¼Œå½¢æˆå·¨å¤§çš„ J çŸ©é˜µ
    J = torch.cat(J_list, dim=1)
    return r, J

# 4.2 è¾…åŠ©å‡½æ•°ï¼šæŠŠæ‰å¹³çš„æ›´æ–°å‘é‡åŠ å›å‚æ•°å­—å…¸
def update_params(params, delta_theta_flat):
    new_params = {}
    idx = 0
    for name, val in params.items():
        numel = val.numel() # å‚æ•°é‡Œçš„å…ƒç´ ä¸ªæ•°
        # ä»æ‰å¹³å‘é‡é‡Œåˆ‡å‡ºä¸€å—
        delta_slice = delta_theta_flat[idx : idx + numel]
        # æ¢å¤å½¢çŠ¶å¹¶ç›¸åŠ 
        new_params[name] = val + delta_slice.view(val.shape)
        idx += numel
    return new_params

# --- 4.3 ä¸»å¾ªç¯ (Main Loop) ---
print("\nğŸš€ å¼€å§‹ LM ä¼˜åŒ– (Phase 2)...")

# è¶…å‚æ•°è®¾ç½® (å‚è€ƒ PPT ç»éªŒå€¼)
mu = 1e-1          # åˆå§‹é˜»å°¼å› å­ (Damping Factor)
div_factor = 3.0   # æˆåŠŸæ—¶ mu å‡å°çš„æ¯”ä¾‹
mul_factor = 2.0   # å¤±è´¥æ—¶ mu å¢å¤§çš„æ¯”ä¾‹
max_iter = 100     # è¿­ä»£æ¬¡æ•° (LM æ”¶æ•›å¾ˆå¿«ï¼Œé€šå¸¸ä¸éœ€è¦å‡ åƒæ¬¡)

# è®°å½• Loss
loss_history = []

for i in range(max_iter):
    # 1. è®¡ç®—å½“å‰çš„ r å’Œ J
    r, J = get_r_and_J(params, x_pde)
    
    # è®¡ç®—å½“å‰ Loss (MSE = mean(r^2))
    mse_loss = torch.mean(r**2)
    loss_history.append(mse_loss.item())
    
    # 2. æ„å»ºçº¿æ€§æ–¹ç¨‹ç³»ç»Ÿ: (J.T @ J + mu * I) @ delta_theta = -J.T @ r
    # H_approx = J.T @ J (é«˜æ–¯ç‰›é¡¿è¿‘ä¼¼æµ·æ£®çŸ©é˜µ)
    H = J.T @ J 
    # g = J.T @ r (æ¢¯åº¦)
    g = J.T @ r
    
    # 3. å°è¯•æ›´æ–° (Trial Step)
    # è¿™æ˜¯ä¸€ä¸ªå¾ªç¯ï¼Œå¦‚æœ Loss å˜å¤§ï¼Œå°±è¦å¢å¤§ mu é‡è¯•ï¼Œç›´åˆ° Loss å˜å°
    step_success = False
    current_try = 0
    
    while not step_success and current_try < 5:
        # A = H + mu * I (åŠ é˜»å°¼)
        # P æ˜¯å‚æ•°æ€»æ•° (501)
        P = H.shape[0]
        I = torch.eye(P).to(device).double()
        A = H + mu * I
        
        # è§£çº¿æ€§æ–¹ç¨‹ A * delta = -g
        # ä½¿ç”¨ torch.linalg.solve æ¯”æ±‚é€†æ›´å‡†æ›´ç¨³
        delta_theta = torch.linalg.solve(A, -g)
        
        # å¾—åˆ°è¯•æ¢æ€§çš„æ–°å‚æ•°
        trial_params = update_params(params, delta_theta)
        
        # è®¡ç®—æ–° Loss
        r_new = batch_residual_fn(trial_params, x_pde)
        mse_loss_new = torch.mean(r_new**2)
        
        # 4. åˆ¤æ–­æ˜¯å¦æ¥å— (Accept/Reject)
        if mse_loss_new < mse_loss:
            # æˆåŠŸï¼æ¥å—å‚æ•°
            params = trial_params
            # è°ƒå° mu (èƒ†å­å¤§ä¸€ç‚¹ï¼Œä»¥æ­¤é€¼è¿‘é«˜æ–¯ç‰›é¡¿)
            mu = mu / div_factor
            step_success = True
            print(f"Iter {i:3d} | âœ… Loss: {mse_loss.item():.8f} -> {mse_loss_new.item():.8f} | mu: {mu:.1e}")
        else:
            # å¤±è´¥ï¼æ‹’ç»å‚æ•°
            # è°ƒå¤§ mu (èƒ†å­å°ä¸€ç‚¹ï¼Œå›å½’æ¢¯åº¦ä¸‹é™)
            mu = mu * mul_factor
            current_try += 1
            # print(f"      | âš ï¸ å°è¯•å¤±è´¥ï¼Œå¢åŠ é˜»å°¼ mu -> {mu:.1e}")

    if mse_loss.item() < 1e-8:
        print("ğŸ‰ è¾¾åˆ°æé«˜ç²¾åº¦ï¼Œæå‰åœæ­¢ï¼")
        break

print("\nâœ… LM ä¼˜åŒ–å®Œæˆï¼")
print(f"   - æœ€ç»ˆ Loss: {loss_history[-1]:.10f}")

# ç”»ä¸ªå›¾çœ‹çœ‹
plt.figure()
plt.semilogy(loss_history)
plt.title("LM Optimization Convergence")
plt.xlabel("Iteration")
plt.ylabel("MSE Loss (Log Scale)")
plt.savefig("lm_convergence.png")
print("ğŸ“Š æ”¶æ•›å›¾å·²ä¿å­˜: lm_convergence.png")