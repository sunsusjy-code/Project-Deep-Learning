import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ•µï¸ ä½¿ç”¨è®¾å¤‡: {device}")

# ==========================================
# ç¬¬ä¸€é˜¶æ®µï¼šä¸Šå¸æ¨¡å¼ (ç”Ÿæˆé«˜è´¨é‡çš„â€œæ¿€æ³¢â€æ•°æ®)
# ==========================================
print("\nğŸ¤– é˜¶æ®µä¸€ï¼šä¸Šå¸æ¨¡å¼ - ç”Ÿæˆè§‚æµ‹æ•°æ®...")
# âš ï¸ ä¿®æ­£ç‚¹ï¼šæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé«˜è´¨é‡çš„ Teacherï¼Œå¦åˆ™ä¾¦æ¢ä¼šè¢«è¯¯å¯¼ï¼

class TeacherNet(nn.Module):
    def __init__(self):
        super().__init__()
        # Teacher ç½‘ç»œå¯ä»¥ç¨å¾®å®½ä¸€ç‚¹ï¼Œç¡®ä¿èƒ½æ‹Ÿåˆæ¿€æ³¢
        self.net = nn.Sequential(
            nn.Linear(2, 20), nn.Tanh(), 
            nn.Linear(20, 20), nn.Tanh(), 
            nn.Linear(20, 20), nn.Tanh(), 
            nn.Linear(20, 1)
        )
    def forward(self, x, t):
        return self.net(torch.cat([x, t], dim=1))

teacher = TeacherNet().to(device)
# å­¦ä¹ ç‡å¯ä»¥å…ˆå¤§åå°ï¼Œè¿™é‡Œç®€å•å¤„ç†
optim_teacher = torch.optim.Adam(teacher.parameters(), lr=0.005)
real_nu = 0.01 / np.pi

print("   -> æ­£åœ¨ä¸¥æ ¼è®­ç»ƒ Teacher (è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´)...")

# âš ï¸ ä¿®æ­£ç‚¹ï¼šå¢åŠ è®­ç»ƒæ¬¡æ•°åˆ° 5000ï¼Œç¡®ä¿æ¿€æ³¢å‡ºç°ï¼
for i in range(5001): 
    x = (torch.rand(2000, 1)*2-1).to(device).requires_grad_(True)
    t = torch.rand(2000, 1).to(device).requires_grad_(True)
    
    # ç‰©ç† Loss
    u = teacher(x, t)
    grads = torch.autograd.grad(u, [x, t], torch.ones_like(u), create_graph=True)
    u_x, u_t = grads[0], grads[1]
    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]
    f = u_t + u*u_x - real_nu*u_xx
    loss_f = torch.mean(f**2)
    
    # IC Loss (t=0)
    x_ic = (torch.rand(500, 1)*2-1).to(device)
    t_ic = torch.zeros(500, 1).to(device)
    u_ic = -torch.sin(np.pi * x_ic)
    loss_ic = torch.mean((teacher(x_ic, t_ic) - u_ic)**2)
    
    # BC Loss (x=-1, 1)
    x_bc = torch.vstack([torch.ones(200, 1), -torch.ones(200, 1)]).to(device)
    t_bc = torch.rand(400, 1).to(device)
    loss_bc = torch.mean((teacher(x_bc, t_bc))**2)

    # âš ï¸ ä¿®æ­£ç‚¹ï¼šç»™ IC/BC æå¤§çš„æƒé‡ï¼Œå¼ºè¿« Teacher å­¦ä¼šå¯¹
    loss = loss_f + 20.0 * loss_ic + 20.0 * loss_bc
    
    optim_teacher.zero_grad(); loss.backward(); optim_teacher.step()
    
    if i % 1000 == 0:
        print(f"      Teacher Iter {i}, Loss: {loss.item():.4f}")

# --- ç”Ÿæˆè§‚æµ‹æ•°æ® ---
num_obs = 1000 # å¢åŠ è§‚æµ‹ç‚¹æ•°é‡
x_obs = (torch.rand(num_obs, 1) * 2 - 1).to(device)
t_obs = torch.rand(num_obs, 1).to(device)
with torch.no_grad():
    u_obs = teacher(x_obs, t_obs) 

print(f"   -> ç”Ÿæˆäº† {num_obs} ä¸ªé«˜è´¨é‡è§‚æµ‹ç‚¹ï¼")


# ==========================================
# ç¬¬äºŒé˜¶æ®µï¼šä¾¦æ¢æ¨¡å¼ (Project B æ ¸å¿ƒ)
# ==========================================
print("\nğŸ•µï¸ é˜¶æ®µäºŒï¼šä¾¦æ¢æ¨¡å¼ - å¼€å§‹åæ¨å‚æ•°...")

class InversePhysicsNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(2, 20), nn.Tanh(), nn.Linear(20, 20), nn.Tanh(), nn.Linear(20, 20), nn.Tanh(), nn.Linear(20, 1)
        )
        # åˆå§‹çŒœæµ‹ nu = exp(-1.0) â‰ˆ 0.36 (æ•…æ„çŒœé”™ï¼ŒçŒœå¾—å¾ˆå¤§)
        self.nu_log = nn.Parameter(torch.tensor(-1.0)) 

    def forward(self, x, t):
        return self.linear_relu_stack(torch.cat([x, t], dim=1))
    
    def get_nu(self):
        return torch.exp(self.nu_log)

def compute_inverse_loss(model, x, t, x_obs, t_obs, u_obs):
    # 1. ç‰©ç† Loss (å’Œä¹‹å‰ä¸€æ ·)
    u = model(x, t)
    grads = torch.autograd.grad(u, [x, t], torch.ones_like(u), create_graph=True)
    u_x, u_t = grads[0], grads[1]
    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]
    
    current_nu = model.get_nu()
    f = u_t + u * u_x - current_nu * u_xx
    loss_f = torch.mean(f**2)
    
    # 2. æ•°æ® Loss (å…³é”®ï¼æ–°å¢çš„ï¼) ğŸš¨
    # ç½‘ç»œé¢„æµ‹çš„è§‚æµ‹ç‚¹æ•°æ®ï¼Œå¿…é¡»å’Œæˆ‘ä»¬ç»™çš„â€œçº¿ç´¢â€ä¸€è‡´
    u_pred_obs = model(x_obs, t_obs)
    loss_data = torch.mean((u_pred_obs - u_obs)**2)
    
    return loss_f, loss_data

# å‡†å¤‡ PDE é…ç‚¹
t_pde = torch.rand(2000, 1).to(device).requires_grad_(True)
x_pde = (torch.rand(2000, 1) * 2 - 1).to(device).requires_grad_(True)

model = InversePhysicsNetwork().to(device)

# === ä¿®æ”¹å‰ ===
#optimizer = torch.optim.Adam(model.parameters(), lr=0.005) # ç¨å¾®è°ƒå¤§ä¸€ç‚¹å­¦ä¹ ç‡
# === ä¿®æ”¹åï¼šç»™ Nu å¼€ä¸ªâ€œVIP åŠ é€Ÿé€šé“â€ ===
# 1. æŠŠå‚æ•°åˆ†æˆä¸¤ç»„ï¼šä¸€ç»„æ˜¯ nuï¼Œä¸€ç»„æ˜¯ç½‘ç»œæƒé‡
nu_params = [model.nu_log]
net_params = [p for p in model.parameters() if p is not model.nu_log]

# 2. ç»™ nu è®¾ç½® 10 å€çš„å­¦ä¹ ç‡ (0.05)ï¼Œç½‘ç»œæƒé‡ä¿æŒ 0.005
optimizer = torch.optim.Adam([
    {'params': net_params, 'lr': 0.005},
    {'params': nu_params, 'lr': 0.05}  # ğŸ”¥ çŒ›è¸©æ²¹é—¨ï¼
])

history_nu = []
real_target = 0.01 / np.pi

print(f"ğŸ¯ çœŸå® Nu: {real_target:.6f}")
print(f"ğŸ¬ åˆå§‹ Nu: {model.get_nu().item():.6f}")

for i in range(8000):#3. é¡ºä¾¿æŠŠè®­ç»ƒæ¬¡æ•°åŠ åˆ° 5000 æˆ– 8000
    optimizer.zero_grad()
    
    loss_f, loss_data = compute_inverse_loss(model, x_pde, t_pde, x_obs, t_obs, u_obs)
    
    # æ€» Loss = ç‰©ç†è¯¯å·® + æ•°æ®è¯¯å·® (ç»™æ•°æ®è¯¯å·®åŠ ä¸ªæƒé‡ï¼Œè®©å®ƒé‡è§†çº¿ç´¢)
    total_loss = loss_f + 100 * loss_data
    
    total_loss.backward()
    optimizer.step()
    
    current_nu = model.get_nu().item()
    history_nu.append(current_nu)
    
    if i % 200 == 0:
        err = abs(current_nu - real_target)/real_target * 100
        print(f"Iter {i}, Loss: {total_loss.item():.4f}, DataLoss: {loss_data.item():.4f}, ğŸ•µï¸ Nu: {current_nu:.6f} (Err: {err:.1f}%)")

# ç”»å›¾
plt.figure()
plt.plot(history_nu)
plt.axhline(y=real_target, color='r', linestyle='--', label='True Nu')
plt.title("Detective Progress: Finding Nu")
plt.xlabel("Iter")
plt.ylabel("Nu Value")
plt.legend()
plt.savefig("nu_fixed.png")
print("âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜ä¸º nu_fixed.png")